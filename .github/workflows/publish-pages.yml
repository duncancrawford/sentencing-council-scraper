name: Publish data to Supabase Storage

on:
  workflow_dispatch:
  schedule:
    - cron: "0 3 * * 1" # Mondays at 03:00 UTC

permissions:
  contents: read

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: 3.13
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Scrape data
        run: |
          python main.py --tab all

      - name: Upload JSON and zipped aggregates to Supabase Storage (S3)
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.SUPABASE_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.SUPABASE_SECRET_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.SUPABASE_REGION }}
          SUPABASE_S3_ENDPOINT: ${{ secrets.SUPABASE_S3_ENDPOINT }}
          SUPABASE_BUCKET: ${{ secrets.SUPABASE_BUCKET }}
          SUPABASE_PREFIX: ${{ secrets.SUPABASE_PREFIX }}
        run: |
          set -euo pipefail
          if [ -z "${AWS_DEFAULT_REGION}" ]; then
            export AWS_DEFAULT_REGION="us-east-1"
          fi
          if [ -z "${SUPABASE_S3_ENDPOINT}" ] || [ -z "${SUPABASE_BUCKET}" ]; then
            echo "Missing SUPABASE_S3_ENDPOINT or SUPABASE_BUCKET secrets."
            exit 1
          fi
          if [ -z "${AWS_ACCESS_KEY_ID}" ] || [ -z "${AWS_SECRET_ACCESS_KEY}" ]; then
            echo "Missing SUPABASE_ACCESS_KEY or SUPABASE_SECRET_KEY secrets."
            exit 1
          fi
          if ! command -v zip >/dev/null 2>&1; then
            echo "zip command not found."
            exit 1
          fi

          STAGING_DIR="$(mktemp -d)"
          export STAGING_DIR
          trap 'rm -rf "${STAGING_DIR}"' EXIT

          # Keep nested JSON files as-is.
          while IFS= read -r -d '' f; do
            rel="${f#data/}"
            mkdir -p "${STAGING_DIR}/$(dirname "${rel}")"
            cp "${f}" "${STAGING_DIR}/${rel}"
          done < <(find data -mindepth 2 -type f -name "*.json" -print0)

          # Zip root-level JSON aggregates before upload.
          while IFS= read -r -d '' f; do
            b="$(basename "${f}")"
            zip -j -q "${STAGING_DIR}/${b}.zip" "${f}"
          done < <(find data -maxdepth 1 -type f -name "*.json" -print0)

          # Build a manifest for consumers to discover available objects.
          python - <<'PY'
          import datetime as dt
          import hashlib
          import json
          import os

          staging_dir = os.environ["STAGING_DIR"]
          entries = []

          for root, _, files in os.walk(staging_dir):
              for filename in files:
                  rel = os.path.relpath(os.path.join(root, filename), staging_dir).replace(os.sep, "/")
                  full_path = os.path.join(staging_dir, rel)
                  hasher = hashlib.sha256()
                  with open(full_path, "rb") as f:
                      for chunk in iter(lambda: f.read(1024 * 1024), b""):
                          hasher.update(chunk)
                  entries.append({
                      "key": rel,
                      "size_bytes": os.path.getsize(full_path),
                      "sha256": hasher.hexdigest(),
                  })

          entries.sort(key=lambda item: item["key"])
          manifest = {
              "schema_version": 1,
              "generated_at_utc": dt.datetime.now(dt.timezone.utc).isoformat().replace("+00:00", "Z"),
              "file_count": len(entries),
              "files": entries,
          }

          with open(os.path.join(staging_dir, "manifest.json"), "w", encoding="utf-8") as f:
              json.dump(manifest, f, ensure_ascii=True, separators=(",", ":"))
          PY

          # Force multipart for larger JSONs (Supabase rejects single-part > ~50MB)
          aws configure set default.s3.multipart_threshold 8MB
          aws configure set default.s3.multipart_chunksize 8MB
          DEST="s3://${SUPABASE_BUCKET}"
          if [ -n "${SUPABASE_PREFIX:-}" ]; then
            DEST="${DEST}/${SUPABASE_PREFIX}"
          fi
          aws s3 sync "${STAGING_DIR}" "${DEST}" \
            --exclude "*" \
            --include "*.json" \
            --include "*.zip" \
            --endpoint-url "${SUPABASE_S3_ENDPOINT}" \
            --delete
